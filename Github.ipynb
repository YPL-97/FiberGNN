{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c509-53d0-46d3-bf9a-1e0af251765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "import glob, random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv,GATConv,SAGEConv, global_mean_pool, global_max_pool, global_add_pool,GINEConv,SGConv,MessagePassing\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from egnn_pytorch import EGNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_scatter import scatter_mean\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)            \n",
    "    np.random.seed(seed)          \n",
    "    torch.manual_seed(seed)       \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False     \n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "graph_file_pattern = \"dataset1/graph_*.csv\"\n",
    "parameter_file_pattern = \"dataset1/parameter_*.csv\"\n",
    "\n",
    "\n",
    "\n",
    "shape_encoder = OneHotEncoder(sparse_output=False)\n",
    "material_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "node_feature_scaler = MinMaxScaler()\n",
    "graph_attribute_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "shape_encoder.fit(np.array(['circle', 'square', 'ellipse']).reshape(-1, 1))\n",
    "material_encoder.fit(np.array(['SiO2', 'ZBLAN', 'GeO2']).reshape(-1, 1))\n",
    "def create_graph_dataset(graph_file_pattern, parameter_file_pattern, device=device):\n",
    "    structure_files = [f for f in glob.glob(graph_file_pattern) ]\n",
    "    parameter_files = [f for f in glob.glob(parameter_file_pattern) ]\n",
    "\n",
    "    all_node_features, all_graph_attributes, all_targets, all_filenames = [], [], [], []\n",
    "    \n",
    "    for struct_file, param_file in zip(structure_files, parameter_files):\n",
    "        df_structure = pd.read_csv(struct_file)\n",
    "        node_features = df_structure.iloc[:, [2, 3, 4, 5]].values\n",
    "        node_shapes = df_structure.iloc[:, 6].astype(str).values.reshape(-1, 1)\n",
    "        node_shapes_encoded = shape_encoder.transform(node_shapes)\n",
    "        node_features = np.hstack((node_features, node_shapes_encoded))\n",
    "        all_node_features.append(node_features)\n",
    "\n",
    "        \n",
    "        df_params = pd.read_csv(param_file)\n",
    "        graph_attributes = df_params.iloc[:, [2, 3]].values\n",
    "        fiber_material = df_params.iloc[:, 1].astype(str).values.reshape(-1, 1)\n",
    "        fiber_material_encoded = material_encoder.transform(fiber_material)\n",
    "        graph_attributes = np.hstack((graph_attributes, fiber_material_encoded))\n",
    "        all_graph_attributes.append(graph_attributes)\n",
    "\n",
    "        \n",
    "        targets = df_params.iloc[: ,6:11].values\n",
    "        all_targets.append(targets)\n",
    "\n",
    "        all_filenames.append((struct_file, param_file))\n",
    "        \n",
    "    all_node_features = np.vstack(all_node_features)\n",
    "    node_feature_scaler.fit(all_node_features)\n",
    "\n",
    "    all_graph_attributes = np.vstack(all_graph_attributes)\n",
    "    graph_attribute_scaler.fit(all_graph_attributes)\n",
    "\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    target_scaler.fit(all_targets)\n",
    "\n",
    "    all_graphs = []\n",
    "    for struct_file, param_file, filename in zip(structure_files, parameter_files, all_filenames):\n",
    "        df_structure = pd.read_csv(struct_file)\n",
    "        node_features = df_structure.iloc[:, [2,3, 4, 5]].values\n",
    "        node_shapes = df_structure.iloc[:, 6].astype(str).values.reshape(-1, 1)\n",
    "        node_shapes_encoded = shape_encoder.transform(node_shapes)\n",
    "        node_features = np.hstack((node_features, node_shapes_encoded))\n",
    "        node_features_normalized = node_feature_scaler.transform(node_features)\n",
    "        node_features_tensor = torch.tensor(node_features_normalized, dtype=torch.float).to(device)\n",
    "        df_params = pd.read_csv(param_file)\n",
    "        graph_attributes = df_params.iloc[:, [2, 3]].values\n",
    "        fiber_material = df_params.iloc[:, 1].astype(str).values.reshape(-1, 1)\n",
    "        fiber_material_encoded = material_encoder.transform(fiber_material)\n",
    "        graph_attributes = np.hstack((graph_attributes, fiber_material_encoded))\n",
    "        graph_attributes_normalized = graph_attribute_scaler.transform(graph_attributes)\n",
    "        graph_attributes_tensor = torch.tensor(graph_attributes_normalized, dtype=torch.float).to(device)\n",
    "        targets = df_params.iloc[:, 6:11].values\n",
    "        targets_normalized = target_scaler.transform(targets)\n",
    "        targets_tensor = torch.tensor(targets_normalized, dtype=torch.float).to(device)\n",
    "        coordinates = df_structure.iloc[:, [0, 1]].values  \n",
    "        num_nodes = coordinates.shape[0] \n",
    "        edge_index, edge_weights = create_edges_from_coordinates(coordinates)\n",
    "\n",
    "        for i in range(graph_attributes_tensor.shape[0]):\n",
    "            graph = Data(\n",
    "                x=node_features_tensor,\n",
    "                edge_index=edge_index,\n",
    "                edge_weights = edge_weights,\n",
    "                y=targets_tensor[i],\n",
    "                graph_attr=graph_attributes_tensor[i],\n",
    "                filename=filename \n",
    "            )\n",
    "            all_graphs.append(graph)\n",
    "    return all_graphs\n",
    "\n",
    "\n",
    "def create_no_edge_graph(num_nodes):\n",
    "    edge_index = torch.empty(2, 0, dtype=torch.long)  \n",
    "    edge_weights = torch.empty(0, dtype=torch.float)  \n",
    "    return edge_index, edge_weights\n",
    "\n",
    "def create_fully_connected_graph(num_nodes):\n",
    "    # 对于全连接图，所有节点都相互连接\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:  \n",
    "                edge_index.append([i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() \n",
    "    edge_weights = torch.ones(edge_index.shape[1])  \n",
    "    return edge_index, edge_weights\n",
    "    \n",
    "def create_circular_graph(num_nodes):\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        edge_index.append([i, (i + 1) % num_nodes])  \n",
    "        edge_index.append([(i + 1) % num_nodes, i])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() \n",
    "    edge_weights = torch.ones(edge_index.shape[1]) \n",
    "    return edge_index, edge_weights\n",
    "\n",
    "def create_random_graph(num_nodes, edge_prob=0.2):\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if random.random() < edge_prob:  \n",
    "                edge_index.append([i, j])\n",
    "                edge_index.append([j, i])  \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  \n",
    "    edge_weights = torch.ones(edge_index.shape[1]) \n",
    "    return edge_index, edge_weights\n",
    "\n",
    "def create_epsilon_neighborhood_graph(coordinates, epsilon = 15):\n",
    "    if not isinstance(coordinates, torch.Tensor):\n",
    "        coordinates = torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "    num_nodes = coordinates.size(0)\n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            dist = torch.norm(coordinates[i] - coordinates[j])\n",
    "            if dist < epsilon:  \n",
    "                edge_index.append([i, j])\n",
    "                edge_index.append([j, i])  \n",
    "                edge_weights.append(dist.item())  \n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  \n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "\n",
    "    return edge_index, edge_weights\n",
    "    \n",
    "def create_edges_from_coordinates(coordinates, k=6):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(coordinates)\n",
    "    dist_matrix, indices = neigh.kneighbors(coordinates)\n",
    "\n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for i in range(len(coordinates)):\n",
    "        for j, dist in zip(indices[i], dist_matrix[i]):\n",
    "            if i != j:  \n",
    "                edge_index.append([i, j])\n",
    "                edge_weights.append(dist)\n",
    "                \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  \n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "    return edge_index, edge_weights\n",
    "\n",
    "# 数据集划分\n",
    "def split_dataset(graphs):\n",
    "    train_data, test_data = train_test_split(graphs, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "all_graph_data = create_graph_dataset(graph_file_pattern, parameter_file_pattern)\n",
    "\n",
    "train_graphs, val_graphs, test_graphs = split_dataset(all_graph_data)\n",
    "print(f\"Train graphs: {len(train_graphs)}, Val graphs: {len(val_graphs)}, Test graphs: {len(test_graphs)}\")\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, output_dim, num_graph_attributes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.num_graph_attributes = num_graph_attributes\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim + num_graph_attributes, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x, edge_index, batch, graph_attr):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)  \n",
    "        graph_attr = graph_attr.view(-1, self.num_graph_attributes)  \n",
    "        x = torch.cat((x, graph_attr), dim=1) \n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, output_dim, num_graph_attributes):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.attn1 = GATConv(num_node_features, hidden_dim)\n",
    "        self.attn2 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.attn3 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim + num_graph_attributes, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, graph_attr):    \n",
    "        x = self.attn1(x, edge_index)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.attn2(x, edge_index)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.attn3(x, edge_index)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch) \n",
    "\n",
    "        graph_attr = graph_attr.view(-1, num_graph_attributes) \n",
    "\n",
    "        x = torch.cat((x, graph_attr), dim=1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, output_dim, num_graph_attributes):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_node_features, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.conv2 = GINConv(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.conv3 = GINConv(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim + num_graph_attributes, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, graph_attr):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.norm1(x)  \n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.norm2(x)  \n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.norm3(x)  \n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)  \n",
    "\n",
    "        graph_attr = graph_attr.view(-1, num_graph_attributes)  \n",
    "        x = torch.cat((x, graph_attr), dim=1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, output_dim, num_graph_attributes):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim + num_graph_attributes, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, graph_attr):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  \n",
    "    \n",
    "        graph_attr = graph_attr.view(-1, num_graph_attributes)\n",
    "        \n",
    "        # 拼接 x 和 graph_attr\n",
    "        x = torch.cat((x, graph_attr), dim=1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, num_epochs):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = 10000  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)            \n",
    "            target = batch.y.view(-1, 5)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)\n",
    "                target = batch.y.view(-1, 5)\n",
    "                loss = criterion(out, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Epoch {epoch+1}: Validation loss improved to {avg_val_loss:.4f}, saving model...\")\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Training complete. Final model saved to {final_model_path}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_list, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_loss_list, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Train and Validation Loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    \n",
    "    train_loss_list_scaled = [loss * 1e3 for loss in train_loss_list]\n",
    "    test_loss_list_scaled = [loss * 1e3 for loss in test_loss_list]\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_list_scaled, label='Train')  \n",
    "    plt.plot(range(1, num_epochs + 1), test_loss_list_scaled, label='Validation') \n",
    "    plt.xlabel('Epoch', fontsize=20)\n",
    "    plt.ylabel('Loss (× 10$^{-3})$', fontsize=20)\n",
    "    plt.title('Train and Validation Loss', fontsize=20)\n",
    "    plt.tick_params(axis='both', labelsize=20, direction='in')\n",
    "    plt.legend(fontsize=20)\n",
    "    \n",
    "    plt.savefig(f\"Train and Validation Loss.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    loss_data = {\n",
    "        'Epoch': list(range(1, num_epochs + 1)),\n",
    "        'Train Loss': train_loss_list,\n",
    "        'Test Loss': test_loss_list\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(loss_data)\n",
    "    df.to_excel('train_and_test_loss.xlsx', index=False, engine='openpyxl')\n",
    "    print(\"Loss data saved to 'train_and_test_loss.xlsx'.\")\n",
    "    \n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    total_mae = 0\n",
    "    total_rmse = 0\n",
    "    total_r2 = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)\n",
    "            target = batch.y.view(-1, 5)\n",
    "            if target.shape[0] != out.shape[0]:\n",
    "                print(f\"Warning: target size {target.shape} does not match output size {out.shape}.\")\n",
    "                continue\n",
    "            # 计算 SmoothL1Loss\n",
    "            loss = criterion(out, target)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            # 计算 MAE 和 RMSE\n",
    "            mae = torch.mean(torch.abs(out - target))\n",
    "            rmse = torch.sqrt(torch.mean((out - target) ** 2))\n",
    "            total_mae += mae.item()\n",
    "            total_rmse += rmse.item()\n",
    "            # 计算 R²\n",
    "            r2 = r2_score(target.cpu().numpy(), out.cpu().numpy())\n",
    "            total_r2 += r2\n",
    "    avg_valid_loss = valid_loss / len(val_loader)\n",
    "    avg_mae = total_mae / len(val_loader)\n",
    "    avg_rmse = total_rmse / len(val_loader)\n",
    "    avg_r2 = total_r2 / len(val_loader)  # 计算平均 R²\n",
    "    print(f\"Validation Loss: {avg_valid_loss}, MAE: {avg_mae}, RMSE: {avg_rmse}, R²: {avg_r2}\")\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total_mae = 0\n",
    "    total_rmse = 0\n",
    "    total_r2 = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)\n",
    "\n",
    "            target = batch.y.view(-1, 5)\n",
    "            if target.shape[0] != out.shape[0]:\n",
    "                print(f\"Warning: target size {target.shape} does not match output size {out.shape}.\")\n",
    "                continue\n",
    "            loss = criterion(out, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # 计算 MAE 和 RMSE\n",
    "            mae = torch.mean(torch.abs(out - target))\n",
    "            rmse = torch.sqrt(torch.mean((out - target) ** 2))\n",
    "            total_mae += mae.item()\n",
    "            total_rmse += rmse.item()\n",
    "\n",
    "            r2 = r2_score(target.cpu().numpy(), out.cpu().numpy())\n",
    "            total_r2 += r2\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_mae = total_mae / len(test_loader)\n",
    "    avg_rmse = total_rmse / len(test_loader)\n",
    "    avg_r2 = total_r2 / len(test_loader)  \n",
    "    \n",
    "    print(f\"Test Loss: {avg_test_loss}, MAE: {avg_mae}, RMSE: {avg_rmse}, R²: {avg_r2}\")\n",
    "\n",
    "num_graph_attributes = 5\n",
    "input_dim = train_graphs[0].num_node_features + num_graph_attributes  \n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n",
    "\n",
    "best_model_path = \"best_model.pth\"\n",
    "final_model_path = 'final_model.pth'\n",
    "\n",
    "model = GCNModel(\n",
    "    num_node_features=train_graphs[0].num_node_features, \n",
    "    hidden_dim=64, \n",
    "    output_dim=train_graphs[0].y.size(0),  \n",
    "    num_graph_attributes=train_graphs[0].graph_attr.size(0),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "start_train_time = time.time()\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs=50)\n",
    "end_train_time = time.time()\n",
    "train_time = end_train_time - start_train_time\n",
    "print(f\"训练时间: {train_time:.2f} 秒\")\n",
    "validate(model, val_loader, criterion)\n",
    "test(model, test_loader, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f78bac-7785-45d1-854a-815759128a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_results(model, test_loader, target_scaler):\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)\n",
    "            all_targets.append(batch.y.cpu().numpy())\n",
    "            all_predictions.append(out.cpu().numpy())\n",
    "    \n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "    num_features = target_scaler.scale_.shape[0]\n",
    "    \n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    \n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    \n",
    "    if all_targets.ndim == 1 or all_targets.shape[1] != num_features:\n",
    "        all_targets = target_scaler.inverse_transform(all_targets.reshape(-1, num_features))\n",
    "        all_predictions = target_scaler.inverse_transform(all_predictions.reshape(-1, num_features))\n",
    "    else:\n",
    "        all_targets = target_scaler.inverse_transform(all_targets)\n",
    "        all_predictions = target_scaler.inverse_transform(all_predictions)\n",
    "  \n",
    "    param_names = [\n",
    "        \"Effective Refractive Index (a.u.)\",\n",
    "        \"Effective Mode Area (um$^2$)\",\n",
    "        \"Nonlinear Coefficient (1/W/km)\",\n",
    "        \"Dispersion Coefficient (ps/nm/km)\",\n",
    "        \"Group velocity dispersion (ps$^2$/km)\",\n",
    "        \"Loss\",\n",
    "        r\"$\\beta_3$\",\n",
    "        r\"$\\beta_4$\"\n",
    "    ]  \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Predictions (1)': all_targets[:, 0],\n",
    "        'Ground Truths (1)': all_predictions[:, 0],\n",
    "        'Predictions (2)': all_targets[:, 1],\n",
    "        'Ground Truths (2)': all_predictions[:, 1],\n",
    "        'Predictions (3)': all_targets[:, 2],\n",
    "        'Ground Truths (3)': all_predictions[:, 2],\n",
    "        'Predictions (4)': all_targets[:, 3],\n",
    "        'Ground Truths (4)': all_predictions[:, 3],\n",
    "        'Predictions (5)': all_targets[:, 4],\n",
    "        'Ground Truths (5)': all_predictions[:, 4],\n",
    "    })\n",
    "    \n",
    "    df.to_excel('predictions_and_ground_truths1.xlsx', index=False)\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(all_targets[:, i], all_predictions[:, i], alpha=0.5)\n",
    "        plt.plot([all_targets[:, i].min(), all_targets[:, i].max()],\n",
    "                 [all_targets[:, i].min(), all_targets[:, i].max()],\n",
    "                 'r--')\n",
    "        plt.title(f'{param_names[i]}', fontsize=20)\n",
    "        plt.xlabel('FEM', fontsize=20)\n",
    "        plt.ylabel('GNN', fontsize=20)\n",
    "        plt.xlim(all_targets[:, i].min(), all_targets[:, i].max())\n",
    "        plt.ylim(all_targets[:, i].min(), all_targets[:, i].max())\n",
    "        \n",
    "        plt.tick_params(axis='both', labelsize=20, direction='in')\n",
    "        \n",
    "        plt.grid(False)\n",
    "        \n",
    "        plt.savefig(f\"{[i]}.png\")\n",
    "        plt.show()\n",
    "\n",
    "start_predict_time = time.time()\n",
    "visualize_test_results(model, test_loader, target_scaler)\n",
    "end_predict_time = time.time()\n",
    "predict_time = end_predict_time - start_predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dbe1e-574f-4c9f-ba96-8dc9efc684f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_visualize(model, graph_file_pattern, parameter_file_pattern, target_scaler, device=device):\n",
    "    structure_files = [f for f in glob.glob(graph_file_pattern) if \"SiO2\" in f]\n",
    "    parameter_files = [f for f in glob.glob(parameter_file_pattern) if \"SiO2\" in f ]\n",
    "    selected_index = random.randint(0, len(structure_files) - 1)\n",
    "    struct_file = structure_files[selected_index]\n",
    "    param_file = parameter_files[selected_index]\n",
    "    print(f\"Structure file: {struct_file}\")\n",
    "    print(f\"Parameter file: {param_file}\")\n",
    "\n",
    "    df_structure = pd.read_csv(struct_file)\n",
    "    df_params = pd.read_csv(param_file)\n",
    "\n",
    "    node_features = df_structure.iloc[:, [0, 1, 4, 5]].values\n",
    "    node_shapes = df_structure.iloc[:, 6].astype(str).values.reshape(-1, 1)\n",
    "    node_shapes_encoded = shape_encoder.transform(node_shapes)\n",
    "    node_features = np.hstack((node_features, node_shapes_encoded))\n",
    "    node_features_normalized = node_feature_scaler.transform(node_features)\n",
    "    node_features_tensor = torch.tensor(node_features_normalized, dtype=torch.float).to(device)\n",
    "\n",
    "    graph_attributes = df_params.iloc[:, [2, 3]].values\n",
    "    fiber_material = df_params.iloc[:, 1].astype(str).values.reshape(-1, 1)\n",
    "    fiber_material_encoded = material_encoder.transform(fiber_material)\n",
    "    graph_attributes = np.hstack((graph_attributes, fiber_material_encoded))\n",
    "    graph_attributes_normalized = graph_attribute_scaler.transform(graph_attributes)\n",
    "    graph_attributes_tensor = torch.tensor(graph_attributes_normalized, dtype=torch.float).to(device)\n",
    "\n",
    "    targets = df_params.iloc[:, 6:11].values\n",
    "    targets_normalized = target_scaler.transform(targets)\n",
    "    targets_tensor = torch.tensor(targets_normalized, dtype=torch.float).to(device)\n",
    "\n",
    "    wavelengths = df_params.iloc[:, 3].values\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(wavelengths)):\n",
    "        data = Data(\n",
    "            x=node_features_tensor,\n",
    "            edge_index=torch.empty((2, 0), dtype=torch.long).to(device),\n",
    "            graph_attr=graph_attributes_tensor[i].unsqueeze(0),\n",
    "            y=targets_tensor[i].unsqueeze(0)\n",
    "        )\n",
    "        data_list.append(data)\n",
    "\n",
    "    batch = Batch.from_data_list(data_list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch.x, batch.edge_index, batch.batch, batch.graph_attr)\n",
    "        preds = preds.cpu().numpy()\n",
    "        preds = target_scaler.inverse_transform(preds)\n",
    "        targets = batch.y.cpu().numpy()\n",
    "        targets = target_scaler.inverse_transform(targets)\n",
    "        preds[:, 1] -=0\n",
    "        predictions = preds\n",
    "        ground_truths = targets\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Wavelength': wavelengths,\n",
    "        'Predictions (1)': predictions[:, 0],\n",
    "        'Ground Truths (1)': ground_truths[:, 0],\n",
    "        'Predictions (2)': predictions[:, 1],\n",
    "        'Ground Truths (2)': ground_truths[:, 1],\n",
    "        'Predictions (3)': predictions[:, 2],\n",
    "        'Ground Truths (3)': ground_truths[:, 2],\n",
    "        'Predictions (4)': predictions[:, 3],\n",
    "        'Ground Truths (4)': ground_truths[:, 3],\n",
    "        'Predictions (5)': predictions[:, 4],\n",
    "        'Ground Truths (5)': ground_truths[:, 4],\n",
    "    })\n",
    "\n",
    "    df.to_excel('predictions_and_ground_truths.xlsx', index=False)\n",
    "    \n",
    "    param_names = [\"Effective Refractive Index\", \"Effective Mode Area\", \"Nonlinear Coefficient\", \"Dispersion Coefficient\",\"Group velocity dispersion\",\"loss\",\"beta3\",\"beta4\",\"Effective Refractive Index\"]\n",
    "    param_units = [\n",
    "        \"a.u.\",\n",
    "        \"um^2\",\n",
    "        \"1/W/km\",\n",
    "        \"ps/nm/km\",\n",
    "        \"ps^2/km\",\n",
    "        \"dB/m\",\n",
    "        \"s^3/m\",\n",
    "        \"s^4/m\",\n",
    "        \"a.u.\"\n",
    "    ]\n",
    "    for i in range(targets.shape[1]):\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(wavelengths, predictions[:, i], '--', label=f\"GNN\")\n",
    "        plt.plot(wavelengths, ground_truths[:, i], label=f\"FEM\")\n",
    "        plt.xlabel(\"Wavelength (nm)\")\n",
    "        plt.ylabel(f\"{param_names[i]} ({param_units[i]})\")\n",
    "        plt.title(f\"{param_names[i]}\")\n",
    "        if i == 0:\n",
    "            plt.legend(loc='best', fontsize=12)\n",
    "        plt.grid(False)\n",
    "        plt.tick_params(axis='both', direction='in')\n",
    "        plt.savefig(f\"{param_names[i]}.png\")\n",
    "        plt.show()\n",
    "\n",
    "test_and_visualize(\n",
    "    model=model,\n",
    "    graph_file_pattern=graph_file_pattern,\n",
    "    parameter_file_pattern=parameter_file_pattern,\n",
    "    target_scaler=target_scaler,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
